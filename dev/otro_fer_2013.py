# -*- coding: utf-8 -*-
"""OTRO FER 2013

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q3f15gvs1E10w_EnAmAmTlp_tdBIh3hI

# No correr ya esta
"""

!pip install tensorflow

!git clone https://github.com/maximocostaroman/Trabajo-Practico-Integrador-Redes-Neuronales-Profundas.git



!unzip /content/Trabajo-Practico-Integrador-Redes-Neuronales-Profundas/archive.zip

import os
import csv
import shutil
import random

# Carpetas de entrada
original_dirs = ['train', 'test']  # carpetas originales

# Carpetas de salida
base_output_dir = 'images'
train_output_dir = os.path.join(base_output_dir, 'train')
test_output_dir = os.path.join(base_output_dir, 'test')

# Crear carpetas de salida
os.makedirs(train_output_dir, exist_ok=True)
os.makedirs(test_output_dir, exist_ok=True)

# CSV de salida
csv_output = 'archive_labels.csv'

# Recolectar im치genes desde carpetas originales
all_data = []

for split in original_dirs:
    for label in os.listdir(split):
        label_path = os.path.join(split, label)
        if not os.path.isdir(label_path):
            continue
        for i, img_name in enumerate(os.listdir(label_path)):
            src_path = os.path.join(label_path, img_name)
            # Crear nombre 칰nico (ignora extensi칩n original y usa .png)
            new_filename = f"{split}_{label}_{i:05d}.png"
            all_data.append((src_path, new_filename, label))  # (ruta_origen, nombre_nuevo, etiqueta)

# Mezclar aleatoriamente
random.shuffle(all_data)

# Dividir en train y test (80/20 por defecto)
split_ratio = 0.8
split_index = int(len(all_data) * split_ratio)
train_data = all_data[:split_index]
test_data = all_data[split_index:]

# Preparar CSV
with open(csv_output, mode='w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['filename', 'label', 'split'])

    # Guardar y copiar im치genes para TRAIN
    for src_path, new_filename, label in train_data:
        dst_path = os.path.join(train_output_dir, new_filename)
        shutil.copy(src_path, dst_path)
        writer.writerow([os.path.join('train', new_filename), label, 'train'])

    # Guardar y copiar im치genes para TEST
    for src_path, new_filename, label in test_data:
        dst_path = os.path.join(test_output_dir, new_filename)
        shutil.copy(src_path, dst_path)
        writer.writerow([os.path.join('test', new_filename), label, 'test'])

"""# **CORRER A PARTIR DE ACA!!**"""

import torch
from torch.utils.data import Dataset
from torchvision.io import read_image
from torchvision import tv_tensors
from torchvision.transforms import v2
from torchvision import transforms
import torchvision.transforms.functional as TF
import torch.nn.functional as F
from PIL import Image
import json
import pandas as pd
import numpy as np
import os

class ImageDataset(Dataset):
    def __init__(self, annotations_file, img_dir,
                 mode= "train",split= "train" , transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform

        self.img_labels = self.img_labels[self.img_labels['split'] == split].reset_index(drop=True)
        unique_labels = sorted(self.img_labels['label'].unique())
        self.label_map = {label: i for i, label in enumerate(unique_labels)}
        self.num_classes = len(unique_labels)
        print(f"Label mapping: {self.label_map}")


    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_filename_in_csv = self.img_labels.iloc[idx]['filename']
        img_path = os.path.join(self.img_dir, img_filename_in_csv)
        image = Image.open(img_path).convert('L')  # Asegura que est칠 en modo grayscale

        image = TF.to_tensor(image)
        # Convertir la etiqueta de cadena a su 칤ndice num칠rico
        label_str = self.img_labels.iloc[idx]['label']
        label = self.label_map[label_str]
        # Convertir el 칤ndice a un tensor de tipo long
        label = torch.tensor(label, dtype=torch.long)

        return image, label

# Transformaciones
import torch
from torchvision.transforms import v2

train_transforms = v2.Compose([
    v2.Resize(size=(48,48)),
    # v2.Grayscale(num_output_channels=3),
    v2.ToTensor(),
    v2.ToDtype(dtype=torch.float32, scale=True),
    v2.RandomHorizontalFlip(p=0.5),
    v2.RandomVerticalFlip(p=0.5)
])

test_transforms = v2.Compose([
    v2.Resize(size=(48,48)),
    # v2.Grayscale(num_output_channels=3),
    v2.ToTensor(),
    v2.ToDtype(dtype=torch.float32, scale=True),
    v2.RandomHorizontalFlip(p=0.5),
    v2.RandomVerticalFlip(p=0.5)
])

resize = v2.Resize(size=(48,48))

train_dataset = ImageDataset(
    annotations_file= "/content/archive_labels.csv",
    img_dir= "/content/images",
    split= "train",
    transform= train_transforms
)

test_dataset = ImageDataset(
    annotations_file= "/content/archive_labels.csv",
    img_dir= "/content/images",
    split= "test",
    transform= test_transforms
)

print("Total: ", len(pd.read_csv("/content/archive_labels.csv")))
print("Tama침o del conjunto de entrenamiento:", len(train_dataset))
print("Tama침o del conjunto de prueba:", len(test_dataset))

#Generamos los dataloaders
from torch.utils.data import DataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True,
    num_workers=2
)

# DataLoader para el conjunto de prueba
test_loader = DataLoader(
    test_dataset,
    batch_size=32,
    shuffle=False,
    num_workers=2
)

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# Ya tienes esta versi칩n mejorada en tu c칩digo m치s reciente, 칰sala:
class FERModel(nn.Module):

    def __init__(self, num_classes=7):
        super(FERModel, self).__init__()

        self.features = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25),

            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25),

            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25)
        )

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256 * 6 * 6, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x
model = FERModel()

model = FERModel()

def train_model(model, train_loader, val_loader, optimizer, num_epochs=10, device='cpu'):
    model = model.to(device)
    train_losses, val_losses = [], []
    train_accuracies, val_accuracies = [], []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = F.cross_entropy(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * images.size(0)

            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        epoch_train_loss = running_loss / len(train_loader.dataset)
        epoch_train_acc = correct / total
        train_losses.append(epoch_train_loss)
        train_accuracies.append(epoch_train_acc)

        # Evaluaci칩n
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = F.cross_entropy(outputs, labels)
                val_loss += loss.item() * images.size(0)

                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        epoch_val_loss = val_loss / len(val_loader.dataset)
        val_accuracy = correct / total

        val_losses.append(epoch_val_loss)
        val_accuracies.append(val_accuracy)

        print(f"Epoch {epoch+1}/{num_epochs} | "
              f"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | "
              f"Val Loss: {epoch_val_loss:.4f} | Val Acc: {val_accuracy:.4f}")

    # Graficar p칠rdidas y accuracy
    plt.figure(figsize=(12,5))

    plt.subplot(1,2,1)
    plt.plot(train_losses, label="Train Loss")
    plt.plot(val_losses, label="Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.title("Training and Validation Loss")

    plt.subplot(1,2,2)
    plt.plot(train_accuracies, label="Train Accuracy", color='blue')
    plt.plot(val_accuracies, label="Validation Accuracy", color='green')
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title("Train & Validation Accuracy")
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Guardar modelo
    torch.save(model.state_dict(), 'modelo_entrenado.pth')

    return train_accuracies, val_accuracies

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FERModel()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

train_model(model, train_loader, test_loader, optimizer, num_epochs=50, device=device)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Etiquetas del FER2013
class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']

# Recolecci칩n de verdaderos y predichos
y_true = []
y_pred = []

model.eval()
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

# Matriz de confusi칩n
cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)

plt.figure(figsize=(8, 6))
disp.plot(cmap='Blues', xticks_rotation=45)
plt.title("Matriz de Confusi칩n")
plt.show()

from PIL import Image
import torchvision.transforms as transforms
import torch.nn.functional as F # Import F

# Load and preprocess the image
image_path = '/content/test/disgust/PrivateTest_11895083.jpg' # Replace with the actual path to your image
image = Image.open(image_path).convert('L') # Convert to grayscale

# Define the same transformations used for the training data
transform = transforms.Compose([
    transforms.Resize((48, 48)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])

image_tensor = transform(image)

model.eval()
image_tensor = image_tensor.unsqueeze(0).to(device)  # agregar dimensi칩n batch
with torch.no_grad():
    outputs = model(image_tensor)  # salidas sin softmax
    probabilities = F.softmax(outputs, dim=1).cpu().numpy()[0]  # convertir a probas

# Mostrar porcentajes de emociones
emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'] # Define emotion_labels
for i, prob in enumerate(probabilities):
    print(f"{emotion_labels[i]}: {prob * 100:.2f}%")

# Mostrar la emoci칩n con mayor porcentaje
max_index = probabilities.argmax()
print(f"\n游녤 Emoci칩n detectada: {emotion_labels[max_index]} ({probabilities[max_index]*100:.2f}%)")